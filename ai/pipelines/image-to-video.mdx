---
title: Image-to-video
---

## Overview

The `image-to-video` pipeline of the AI subnet allows you to generate animated
**high-quality** videos from images. This pipeline is powered by the latest
diffusion models in the HugginFace
[image-to-video](https://huggingface.co/models?pipeline_tag=image-to-video)
pipeline.

<div align="center">

```mermaid
graph LR
    A[
        <div style="width: 200px;">
            <img src="/images/ai/cool-cat-hat.png"/>
        </div>
        ] --> B[Gateway]
    B --> C[Orchestrator]
    C --> B
    B --> D[<div style="width: 200px;"><img src="/images/ai/cool-cat-hat-moving.gif"/></div>]
```

</div>

## Models

### Warm Models

During the **Alpha** phase of the AI Video Subnet, Orchestrators are encouraged
to maintain at least **one model** per pipeline in an active state on their GPUs
(known as "warm models"). This practice is designed to provide quicker response
times for **early builders** on the Subnet. We're working to optimize GPU model
loading/unloading to relax this requirement The current warm model requested for
the `image-to-video` pipeline is:

- [stabilityai/stable-video-diffusion-img2vid-xt-1-1](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1):
  An updated version of the stable-video-diffusion-img2vid-xt model with
  enhanced performance.

<Tip>
  For faster responses with a different
  [image-to-video](https://huggingface.co/models?pipeline_tag=image-to-video)
  diffusion model, ask Orchestrators to load it on their GPU via our `ai-video`
  channel in [Our Discord Server](https://discord.gg/livepeer).
</Tip>

### On-Demand Models

Orchestrators can load any `image-to-video` diffusion model from Hugging Face
on-demand, optimizing GPU resources by only loading models when needed. Although
the subnet is designed to support **any** Hugging Face `image-to-video` model,
during the **Alpha** phase, orchestrators need to pre-download a model. The
following models have been tested and verified for the `image-to-video`
pipeline:

<Note>
  If a specific model you wish to use is not listed, please submit a [feature
  request](https://github.com/livepeer/ai-worker/issues/new?assignees=&labels=enhancement%2Cmodel&projects=&template=model_request.yml)
  on our GitHub. We will verify the model and add it to the list.
</Note>

{/* prettier-ignore */}
<Accordion title="Tested and Verified Diffusion Models">
- [stable-video-diffusion-img2vid-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt):
  A model by Stability AI designed for stable video diffusion from images.
- [stabilityai/stable-video-diffusion-img2vid-xt-1-1](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1):
  An updated version of the stable-video-diffusion-img2vid-xt model with
  enhanced performance.
</Accordion>

## Basic Usage Instructions

<Tip>
  For a detailed understanding of the `image-to-video` endpoint and to
  experiment with the API, see the [AI Subnet API
  Reference](/ai/api-reference/image-to-video).
</Tip>

To generate an image with the `image-to-video` pipeline, send a `POST` request
to the Gateway's `image-to-video` API endpoint:

```bash
curl -X POST "https://<gateway-ip>/image-to-video" \
    -F model_id=stabilityai/stable-video-diffusion-img2vid-xt-1-1 \
    -F image=@<PATH_TO_IMAGE>
```

In this command:

- `<gateway-ip>` should be replaced with your AI Gateway's IP address.
- `model_id` is the diffusion model for image generation.
- The `image` field holds the **absolute** path to the image file to be
  transformed.

For additional optional parameters, refer to the
[AI Subnet API Reference](/ai/api-reference/image-to-video).

After execution, the Orchestrator processes the request and returns the response
to the Gateway:

```json
{
  "images": [
    {
      "seed": 1914955328,
      "url": "/stream/2b835716/01c0e9a6.mp4"
    }
  ]
}
```

The `url` in the response is the URL of the generated image. Download the image
with:

```bash
curl -O "https://<STORAGE_ENDPOINT>/stream/2b835716/01c0e9a6.mp4"
```

## API Reference

<Card
  title="API Reference"
  icon="rectangle-terminal"
  href="/ai/api-reference/image-to-video"
>
  Explore the `image-to-video` endpoint and experiment with the API in the AI
  Subnet API Reference.
</Card>
