---
title: Text-to-Image
---

## Overview

The `text-to-image` pipeline of the AI subnet allows you to generate
**high-quality** images from text descriptions. This pipeline is powered by the
latest diffusion models in the HugginFace
[text-to-image](https://huggingface.co/models?pipeline_tag=text-to-image)
pipeline.

{/* TODO: Replace with relative url when mintlify fixed issue. */}

<div align="center">

```mermaid
graph LR
    A["A cool cat on the beach"] --> B[Gateway]
    B --> C[Orchestrator]
    C --> B
    B --> D[<div style="width: 200px;"><img src="https://mintlify.s3-us-west-1.amazonaws.com/na-36-ai-video/images/ai/cool-cat.png" alt="Image of cool cat"/></div>]
```

</div>

## Models

### Warm Models

During the **Alpha** phase of the AI Video Subnet, Orchestrators are encouraged
to maintain at least **one model** per pipeline in an active state on their GPUs
(known as "warm models"). This practice is designed to provide quicker response
times for **early builders** on the Subnet. We're working to optimize GPU model
loading/unloading to relax this requirement The current warm model requested for
the `text-to-image` pipeline is:

- [ByteDance/SDXL-Lightning](https://huggingface.co/ByteDance/SDXL-Lightning): A
  high-performance diffusion model developed by ByteDance.

<Tip>
  For faster responses with a different
  [text-to-image](https://huggingface.co/models?pipeline_tag=text-to-image)
  diffusion model, ask Orchestrators to load it on their GPU via the `ai-video`
  channel in [Discord Server](https://discord.gg/livepeer).
</Tip>

### On-Demand Models

Orchestrators can load any `text-to-image` diffusion model from Hugging Face
on-demand, optimizing GPU resources by only loading models when needed. Although
the subnet is designed to support **any** Hugging Face `text-to-image` model,
during the **Alpha** phase, orchestrators need to pre-download a model. The
following models have been tested and verified for the `text-to-image` pipeline:

<Note>
  If a specific model you wish to use is not listed, please submit a [feature
  request](https://github.com/livepeer/ai-worker/issues/new?assignees=&labels=enhancement%2Cmodel&projects=&template=model_request.yml)
  on GitHub to get the model verified and added to the list.
</Note>

{/* prettier-ignore */}
<Accordion title="Tested and Verified Diffusion Models">
- [sd-turbo](https://huggingface.co/stabilityai/sd-turbo): A high-performance
  diffusion model by Stability AI.
- [sdxl-turbo](https://huggingface.co/stabilityai/sdxl-turbo): An extended
  version of sd-turbo with enhanced capabilities.
- [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5):
  A stable diffusion model by Runway ML.
- [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0):
  A base model for stable diffusion by Stability AI.
- [openjourney-v4](https://huggingface.co/prompthero/openjourney-v4): A model by
  Prompthero for open-ended journey generation.
- [ByteDance/SDXL-Lightning](https://huggingface.co/ByteDance/SDXL-Lightning): A
  lightning-fast diffusion model by ByteDance.
- [SG161222/RealVisXL_V4.0](https://huggingface.co/SG161222/RealVisXL_V4.0/tree/main):
  A high-quality diffusion model aimed at photorealism.
</Accordion>

## Basic Usage Instructions

<Tip>
  For a detailed understanding of the `text-to-image` endpoint and to experiment
  with the API, see the [AI Subnet API
  Reference](/ai/api-reference/text-to-image).
</Tip>

To generate an image with the `text-to-image` pipeline, send a `POST` request to
the Gateway's `text-to-image` API endpoint:

```bash
curl -X POST "https://<gateway-ip>/text-to-image" \
    -H "Content-Type: application/json" \
    -d '{
        "model_id":"ByteDance/SDXL-Lightning",
        "prompt":"A cool cat on the beach",
        "width": 1024,
        "height": 1024
    }'
```

In this command:

- Replace `<gateway-ip>` with the IP address of your AI Gateway.
- The `model_id` field identifies the diffusion model used for image generation.
- The `prompt` field holds the text description for the image to be generated.

For additional optional parameters, refer to the
[AI Subnet API Reference](/ai/api-reference/text-to-image).

After execution, the Orchestrator processes the request and returns the response
to the Gateway:

```json
{
  "images": [
    {
      "seed": 2562822894,
      "url": "https://<gateway-ip>/stream/d0fc1fc6/8fdf5a94.png"
    }
  ]
}
```

The `url` in the response is the URL of the generated image. Download the image
with:

```bash
curl -O "https://<gateway-ip>/stream/d0fc1fc6/8fdf5a94.png"
```

## API Reference

<Card
  title="API Reference"
  icon="rectangle-terminal"
  href="/ai/api-reference/text-to-image"
>
  Explore the `text-to-image` endpoint and experiment with the API in the AI
  Subnet API Reference.
</Card>
