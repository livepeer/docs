---
openapi: post /llm
---

<Note>
The LLM pipeline is Open AI API compatible but does not implement all features of the Open AI API.

To stream responses set `stream: true` in the request json.  The response is then streamed with Server Sent Events (SSE)
in chunks as the tokens are generated.  

Each streaming response chunk will have the following format:

`data: {"choices": [{"delta":{"content": "...token...", "role":"[user/assisant]"}, "finish_reason":""...."`

The final chunk of the response will be indicated by blank content and finish_reason: "stop":

`data: {"choices": [{"delta":{"content": "", "role":"assisant"}, "finish_reason":"stop"`

The Response type below is for non-streaming responses that will return all of the response in one
</Note>
<Info>
  The default Gateway used in this guide is the public
  [Livepeer.cloud](https://www.livepeer.cloud/) Gateway. It is free to use but
  not intended for production-ready applications. For production-ready
  applications, consider using the [Livepeer Studio](https://livepeer.studio/)
  Gateway, which requires an API token. Alternatively, you can set up your own
  Gateway node or partner with one via the `ai-video` channel on
  [Discord](https://discord.gg/livepeer).
</Info>
