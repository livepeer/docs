---
title: Start your AI Gateway
---

The _AI Subnet_ is not yet integrated into the main
[go-livepeer](https://github.com/livepeer/go-livepeer) software due to its
**Alpha** status. To equip your _Gateway_ with AI inference capabilities, please
use the `ai-video` branch of
[go-livepeer](https://github.com/livepeer/go-livepeer/tree/ai-video). This
branch contains the necessary software for the AI Gateway. Currently, there are
two methods to run the _AI Subnet_ software:

- **Docker**: This is the most straightforward and recommended method to run the
  AI Gateway.
- **Pre-built Binaries**: If you prefer not to use Docker, pre-built binaries
  are available.

## Start the AI Gateway

Please follow the steps below to start your _AI Subnet_ Gateway:

<Tabs>
    <Tab title="Use Docker (Recommended)">
        <Steps>
            <Step title="Retrieve the AI Subnet Docker Image">
                Fetch the latest _AI Subnet_ Docker image from the [Livepeer Docker Hub](https://hub.docker.com/r/livepeer/go-livepeer) with the following command:

                ```bash
                docker pull livepeer/go-livepeer:ai-video
                ```
            </Step>
            <Step title="Launch an (Offchain) AI Gateway">
                Execute the _AI Subnet_ Docker image using the following command:

                ```bash
                docker run \
                    --name livepeer_ai_gateway \
                    -v ~/.lpData2/:/root/.lpData2 \
                    -p 8937:8937 \
                    --network host \
                    livepeer/go-livepeer:ai-video \
                    -datadir ~/.lpData2 \
                    -broadcaster \
                    -orchAddr <ORCH_LIST> \
                    -httpAddr 0.0.0.0:8937 \
                    -v 6 \
                    -httpIngest
                ```

                This launches an **offchain** AI Gateway. The flags are similar to those used for a _Mainnet Transcoding Network Gateway_. For more information, see the [go-livepeer CLI reference](/references/go-livepeer/cli-reference).
            </Step>
            <Step title="Confirm the AI Gateway is Operational">
                If your _AI Subnet_ Gateway is functioning correctly, you should see the following output:

                ```bash
                I0501 11:07:47.609839       1 mediaserver.go:201] Transcode Job Type: [{P240p30fps16x9 600k 30 0 426x240 16:9 0 0 0s 0 0 0 0} {P360p30fps16x9 1200k 30 0 640x360 16:9 0 0 0s 0 0 0 0}]
                I0501 11:07:47.609917       1 mediaserver.go:226] HTTP Server listening on http://0.0.0.0:8937
                I0501 11:07:47.609963       1 lpms.go:92] LPMS Server listening on rtmp://127.0.0.1:1935
                ```
            </Step>
            <Step title="Check Port Availability">
                To make your _AI Subnet_ Gateway accessible from the internet, you need to configure your network settings. Ensure that port `8937` is unblocked on your machine. Additionally, consider setting up port forwarding on your router, which will allow the Gateway node to be reachable from the internet.
            </Step>
        </Steps>
    </Tab>
    <Tab title="Use Binaries">
        <Steps>
            {/* TODO: Simplify this step */}
            <Step title="Download the Latest AI Subnet Binary">
                Visit the [#ðŸª›â”‚builds Channel](https://discord.com/channels/423160867534929930/577736983036559360) on the [Livepeer community Discord](https://discord.com/channels/423160867534929930/577736983036559360). Find the latest message mentioning `Branch: ai-video` and your platform under `Platform:`. This message includes the newest _AI Subnet_ binaries for your system.
            </Step>
            <Step title="Extract and Configure the Binary">
                Once downloaded, extract the binary to a directory of your choice.
            </Step>
            <Step title="Launch an (Offchain) AI Gateway">
                Run the following command to start your _AI Subnet_ Gateway:

                ```bash
                ./livepeer \
                    -datadir ~/.lpData2 \
                    -broadcaster \
                    -orchAddr <ORCH_LIST> \
                    -httpAddr 0.0.0.0:8937 \
                    -v 6 \
                    -httpIngest
                ```

                This command launches an **offchain** AI Gateway. These flags are similar to running a _Mainnet Transcoding Network Gateway_. For more information see the [go-livepeer CLI reference](/references/go-livepeer/cli-reference).
            </Step>
            <Step title="Confirm the AI Gateway is Operational">
                If your _AI Subnet_ Gateway is functioning correctly, you should see the following output:

                ```bash
                I0501 11:07:47.609839       1 mediaserver.go:201] Transcode Job Type: [{P240p30fps16x9 600k 30 0 426x240 16:9 0 0 0s 0 0 0 0} {P360p30fps16x9 1200k 30 0 640x360 16:9 0 0 0s 0 0 0 0}]
                I0501 11:07:47.609917       1 mediaserver.go:226] HTTP Server listening on http://0.0.0.0:8937
                I0501 11:07:47.609963       1 lpms.go:92] LPMS Server listening on rtmp://127.0.0.1:1935
                ```
            </Step>
            <Step title="Check Port Availability">
                To make your _AI Subnet_ Gateway accessible from the internet, you need to configure your network settings. Ensure that port `8937` is unblocked on your machine. Additionally, consider setting up port forwarding on your router, which will allow the Gateway node to be reachable from the internet.
            </Step>
        </Steps>
        <Note>
            If no binaries are available for your system, you can build the [ai-video branch](https://github.com/livepeer/go-livepeer/tree/ai-video) of [go-livepeer](https://github.com/livepeer/go-livepeer) from source by following the instructions in the [Livepeer repository](https://docs.livepeer.org/Gateways/guides/install-go-livepeer) or by reaching out to the Livepeer community on [Discord](https://discord.gg/livepeer).
        </Note>
    </Tab>

</Tabs>

## Confirm the AI Gateway is Operational

After launching the _AI Subnet_ Gateway, verify its operation by sending an AI
inference request directly to it. This requires an active **of-chain** AI
Orchestrator. For guidance on setting up an AI Orchestrator, refer to the
[AI Orchestrator Setup Guide](/ai/orchestrators/get-started).

<Steps>
    <Step title="Launch an AI Orchestrator">
        Start an AI Orchestrator on port `8936` following the [AI Orchestrator Setup Guide](/ai/orchestrators/get-started). Ensure it has loaded the desired model for inference (e.g., "ByteDance/SDXL-Lightning").
    </Step>
    <Step title="Link Gateway to AI Orchestrator">
        To connect your Gateway to the AI Orchestrator, specify the Orchestrator's address when launching the Gateway. Replace `<ORCH_LIST>` with the Orchestrator's address, like so:

        ```bash
        -orchAddr 0.0.0.0:8936
        ```
    </Step>
    <Step title="Submit an Inference Request">
        Refer to the [AI API reference](/ai/api-reference/text-to-image) for the AI Gateway to understand how to submit an inference request. For instance, to generate an image from text, use the following `curl` command:

        ```bash
        curl -X POST "https://0.0.0.0:8937/text-to-image" \
            -H "Content-Type: application/json" \
            -d '{
                "model_id":"ByteDance/SDXL-Lightning",
                "prompt":"A cool cat on the beach",
                "width": 1024,
                "height": 1024
            }'
        ```
    </Step>
    <Step title="Inspect the Response">
        If the Gateway is functioning correctly, you should receive a response similar to the following:

        ```json
        {
            "images": [
                {
                "seed": 2562822894,
                "url": "https://0.0.0.0:8937/stream/d0fc1fc6/8fdf5a94.png"
                }
            ]
        }
        ```

        Consult the [Text-to-image Pipeline Documentation](/ai/pipelines/text-to-image) for more details on interpreting the response.
    </Step>

</Steps>
