---
title: Configuring AI Models
---

Before deploying your AI Orchestrator node on the AI Subnet, you must choose the
AI models for which you want to perform AI inference tasks. This guide assists
in configuring these models. The following page,
[Download AI Models](/ai/orchestrators/models-download), provides instructions
for their download. For details on supported pipelines and models, consult
[Pipelines](/ai/pipelines/text-to-image).

## Configuration File Format

Orchestrators specify supported AI models in an `aiModels.json` file, typically
located in the `~/.lpData` directory. Below is an example configuration showing
currently **recommended** models and their respective prices.

```json
[
  {
    "pipeline": "text-to-image",
    "model_id": "ByteDance/SDXL-Lightning",
    "price_per_unit": 4768371
  },
  {
    "pipeline": "image-to-image",
    "model_id": "ByteDance/SDXL-Lightning",
    "price_per_unit": 4768371
  },
  {
    "pipeline": "upscale",
    "model_id": "stabilityai/stable-diffusion-x4-upscaler",
    "price_per_unit": 4768371
  },
  {
    "pipeline": "audio-to-text",
    "model_id": "openai/whisper-large-v3",
    "price_per_unit": 12882811
  },
  {
    "pipeline": "image-to-video",
    "model_id": "stabilityai/stable-video-diffusion-img2vid-xt-1-1",
    "price_per_unit": 3390842,
    "warm": true,
    "optimization_flags": {
      "SFAST": true,
      "DEEPCACHE": false
    }
  }
]
```

### Key Configuration Fields

<Info>
  During the **Alpha** phase, only one "warm" model per GPU is supported.
</Info>

<ParamField path="pipeline" type="string" required="true">
  The inference pipeline to which the model belongs (e.g., `text-to-image`).
</ParamField>
<ParamField path="model_id" type="string" required="true">
  The [Hugging Face model
  ID](https://huggingface.co/docs/transformers/en/main_classes/model).
</ParamField>
<ParamField path="price_per_unit" type="integer" required="true">
  The price in [Wei](https://ethdocs.org/en/latest/ether.html) per unit, which
  varies based on the pipeline (e.g., per pixel for `image-to-video`).
</ParamField>
<ParamField path="warm" type="boolean">
  If `true`, the model is preloaded on the GPU to decrease runtime.
</ParamField>
<ParamField path="optimization_flags" type="object">
  Optional flags to enhance performance (details below).
</ParamField>

### Optimization Flags

<Warning>
  These flags are still **experimental** and may not always perform as expected.
  If you encounter any issues, please report them to the [go-livepeer
  repository](https://github.com/livepeer/go-livepeer/issues/new/choose).
</Warning>

<Note>At this time, these flags are only compatible with **warm** models.</Note>

Our AI Subnet pipelines offer a suite of optimization flags. These are designed
to enhance the performance of **warm** models by either increasing **inference
speed** or reducing **VRAM** usage. Currently, the following flags are
available:

#### Image-to-video Pipeline Optimization

<ParamField path="SFAST" type="boolean">
  The `SFAST` flag enables the [Stable
  Fast](https://github.com/chengzeyi/stable-fast) optimization framework,
  potentially boosting inference speed by up to **25%** with **no quality
  loss**. Can not be used in conjunction with `DEEPCACHE`.
</ParamField>

#### Text-to-image pipeline Optimization

<Warning>**DO NOT** enable `DEEPCACHE` for Lightning/Turbo models since they're already optimized. Due to [known limitations](https://github.com/horseee/DeepCache/issues/27), it does not provide speed benefits and may significantly lower image quality.</Warning>

<ParamField path="DEEPCACHE" type="boolean">
  The `DEEPCACHE` flag enables the
  [DeepCache](https://github.com/horseee/DeepCache) optimization framework,
  which can enhance inference speed by up to **50%** with **minimal quality
  loss**. The speedup becomes more pronounced as the number of inference steps
  increases. Cannot be used simultaneously with `SFAST`.
</ParamField>
