---
title: Configuring AI Models
---

Before deploying your AI Orchestrator node on the Livepeer AI network, you must
choose the AI models you want to serve for AI inference tasks. This guide
assists in configuring these models. The following page,
[Download AI Models](/ai/orchestrators/models-download), provides instructions
for their download. For details on supported pipelines and models, refer to
[Pipelines](/ai/pipelines/text-to-image).

## Configuration File Format

Orchestrators specify supported AI models in an `aiModels.json` file, typically
located in the `~/.lpData` directory. Below is an example configuration showing
currently **recommended** models and their respective prices.

<Info>
  Pricing used in this example is subject to change and should be set
  competitively based on market research and costs to provide the compute.
</Info>

```json
[
  {
    "pipeline": "text-to-image",
    "model_id": "SG161222/RealVisXL_V4.0_Lightning",
    "price_per_unit": 4768371
  },
  {
    "pipeline": "image-to-image",
    "model_id": "timbrooks/instruct-pix2pix",
    "price_per_unit": 4768371
  },
  {
    "pipeline": "upscale",
    "model_id": "stabilityai/stable-diffusion-x4-upscaler",
    "price_per_unit": 4768371
  },
  {
    "pipeline": "audio-to-text",
    "model_id": "openai/whisper-large-v3",
    "price_per_unit": 12882811,
    "url": "<CONTAINER_URL>:<PORT>",
    "token": "<OPTIONAL_BEARER_TOKEN>",
    "capacity": 1
  },
  {
    "pipeline": "segment-anything-2",
    "model_id": "facebook/sam2-hiera-large",
    "price_per_unit": 3565,
    "pixels_per_unit": 1e13,
    "currency": "USD",
    "warm": true
  },
  {
    "pipeline": "image-to-video",
    "model_id": "stabilityai/stable-video-diffusion-img2vid-xt-1-1",
    "price_per_unit": 3390842,
    "warm": true,
    "optimization_flags": {
      "SFAST": true,
      "DEEPCACHE": false
    }
  },
  {
    "pipeline": "text-to-speech",
    "model_id": "parler-tts/parler-tts-large-v1",
    "price_per_unit": 11,
    "pixels_per_unit": 1e2,
    "currency": "USD"
  }
]
```

### Key Configuration Fields

<Info>
  During the **Beta** phase, only one "warm" model per GPU is supported.
</Info>

<ParamField path="pipeline" type="string" required="true">
  The inference pipeline to which the model belongs (e.g., `text-to-image`).
</ParamField>
<ParamField path="model_id" type="string" required="true">
  The [Hugging Face model
  ID](https://huggingface.co/docs/transformers/en/main_classes/model).
</ParamField>
<ParamField path="price_per_unit" type="integer" required="true">
  The price in [Wei](https://ethdocs.org/en/latest/ether.html) per unit, which
  varies based on the pipeline (e.g., per pixel for `image-to-video`).
</ParamField>
<ParamField path="warm" type="boolean">
  If `true`, the model is preloaded on the GPU to decrease runtime.
</ParamField>
<ParamField path="optimization_flags" type="object">
  Optional flags to enhance performance (details below).
</ParamField>
<ParamField path="url" type="string">
  The URL and port where the model container or custom container manager software is running.
</ParamField>
<ParamField path="token" type="string">
  Optional token required to interact with the model container or custom container manager software.
</ParamField>
<ParamField path="capacity" type="integer">
  Optional capacity of the model. This is the number of inference tasks the model can handle at the same time. This defaults to 1.
</ParamField>

### Optimization Flags

<Warning>
  These flags are still **experimental** and may not always perform as expected.
  If you encounter any issues, please report them to the [go-livepeer
  repository](https://github.com/livepeer/go-livepeer/issues/new/choose).
</Warning>

<Note>At this time, these flags are only compatible with **warm** models.</Note>

The Livepeer AI pipelines offer a suite of optimization flags. These are
designed to enhance the performance of **warm** models by either increasing
**inference speed** or reducing **VRAM** usage. Currently, the following flags
are available:

#### Image-to-video Pipeline Optimization

<ParamField path="SFAST" type="boolean">
  The `SFAST` flag enables the [Stable
  Fast](https://github.com/chengzeyi/stable-fast) optimization framework,
  potentially boosting inference speed by up to **25%** with **no quality
  loss**. Cannot be used in conjunction with `DEEPCACHE`.
</ParamField>

#### Text-to-image Pipeline Optimization

<Warning>
  **DO NOT** enable `DEEPCACHE` for Lightning/Turbo models since they're already
  optimized. Due to [known
  limitations](https://github.com/horseee/DeepCache/issues/27), it does not
  provide speed benefits and may significantly lower image quality.
</Warning>

<ParamField path="DEEPCACHE" type="boolean">
  The `DEEPCACHE` flag enables the
  [DeepCache](https://github.com/horseee/DeepCache) optimization framework,
  which can enhance inference speed by up to **50%** with **minimal quality
  loss**. The speedup becomes more pronounced as the number of inference steps
  increases. Cannot be used simultaneously with `SFAST`.
</ParamField>

### External Containers

<Warning>
  This feature is intended for advanced users. Incorrect setup can lead to a
  lower orchestrator score and reduced fees.
</Warning>

By default, the worker manages the lifecycle of model containers using the
[Docker client Go library](https://pkg.go.dev/github.com/docker/docker/client)
to start and stop containers based on demand. However, if you prefer to use
custom logic, you can specify the `url`, `token`, and `capacity` fields in the
model configuration. This hands over full responsibility to you for ensuring the
advertised models are running behind the specified endpoint. This feature is
intended for advanced users.

#### Important Remarks

- This feature allows you to use any container management software, such as
  [Kubernetes](https://kubernetes.io/), [Podman](https://podman.io/),
  [Docker Swarm](https://docs.docker.com/engine/swarm/), or
  [Nomad](https://www.nomadproject.io/), or custom scripts to manage container
  lifecycles based on request volume.
- The `capacity` field protects orchestrators from being penalized for failed
  jobs. Set it appropriately to avoid suspension due to mishandled requests.
- It is the orchestrator's responsibility to ensure the correct container with
  the correct endpoints is running behind the specified URL. However, we welcome
  feedback to improve this feature, so please reach out to us.
